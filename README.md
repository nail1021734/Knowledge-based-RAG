# Knowledge-based RAG

<p align="center">
  <img src="imgs/example.png" alt="Logo" width="500"/>
</p>

This repository provides a framework for building a knowledge graph from text data and using it for retrieval-augmented generation (RAG) with large language models (LLMs). The process involves extracting entities and relations from text, storing them in a Neo4j database, and retrieving relevant information based on user queries.

## File Structure

```shell
.
├── databases              # Stores data extracted by LLMs
├── data_sources           # Contains source data for relation extraction (Create this folder manually)
├── imgs                   # README images
├── KG_builder             # Scripts for building the knowledge graph
├── neo4j                  # Files related to the Neo4j database (auto-generated by docker-compose)
├── retrievers             # Retrieval methods for GRAG (currently under development)
├── docker-compose.yml     # Script to set up the Neo4j environment
├── README.md              # This file
└── utils                  # Utility functions
```

## Install Requirements

```bash
pip install -r requirements.txt
```

## Set Up Neo4j Database

Make sure you have **Docker** and **Docker Compose** installed.
(*Docker Compose should be included in recent Docker versions.*)

Start the Neo4j database:

```bash
docker compose up -d
```

## Build the Knowledge Graph

### Extract Triplets using LLM

In this step, we use an LLM to extract **entities** and **relations** from text.
The extracted information is stored as triplets: `(subject, relation, object)`, along with the **index of the original text chunk**.

**We use 4-bit quantization by default to reduce GPU usage.**

#### Workflow

1. Load data from a specified directory and split it into text chunks.
2. For each chunk, extract triplets using an LLM and save them to a JSON file.
3. Output includes `clean_triplets.json` and `text_chunk.json`.

| Argument          | Description                                                                            |
| ----------------- | -------------------------------------------------------------------------------------- |
| `data_source_dir` | A folder under `data_sources` containing the source data (currently supports PDF only) |
| `database_dir`    | Name of the output folder (will be created inside `databases`)                         |
| `model_name`      | LLM model for extraction (default: `microsoft/phi-2`)                                  |
| `chunk_size`      | Size of each text chunk (default: 512)                                                 |
| `chunk_overlap`   | Overlap size between text chunks (default: 256)                                        |
| `max_tokens`      | Max number of tokens for LLM generation                                                |
| `temperature`     | Sampling temperature (default: 0.7)                                                    |
| `top_p`           | Top-p sampling parameter (default: 0.9)                                                |
| `top_k`           | Top-k sampling parameter (default: -1)                                                 |
| `device`          | Device to run inference (`cuda` or `cpu`)                                              |

```bash
CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=1 python -m KG_builder.LLM \
  --data_source_dir test \
  --database_dir test
```


### Import Triplets into Neo4j

Use the triplets extracted in the previous step to populate the Neo4j database.

| Argument       | Description                                                                      |
| -------------- | -------------------------------------------------------------------------------- |
| `database_dir` | A folder under `databases` that contains the JSON outputs from the previous step |

```bash
python -m KG_builder.neo4j --database_dir test
```

## Retrieve Information from Knowledge Graph (TODO)

We use a retrieval strategy based on entity recognition from user queries:

1. Identify entities in the query.
2. Retrieve their **n-hop relations** from the graph.
3. Collect the corresponding **original text chunks or their summaries**.
4. Feed the retrieved information into an LLM for generating responses.
